{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalysis(BaseModel):\n",
    "    sentiment: str = Field(description=\"Overall sentiment of the text (positive, negative, or neutral)\")\n",
    "    main_topics: List[str] = Field(description=\"List of main topics in the text\")\n",
    "    word_count: int = Field(description=\"Total word count of the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Claude tool use can be quite useful if used correctly.\n",
      "Analyzing text...\n",
      "\n",
      "Sentiment: positive\n",
      "Main topics: tool use, Claude\n",
      "Word count: 10\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_with_claude(api_key: str, text: str) -> TextAnalysis:\n",
    "    client = Anthropic(api_key=api_key)\n",
    " \n",
    "    text_analysis_schema = TextAnalysis.model_json_schema()\n",
    " \n",
    "    tools = [\n",
    "        {\n",
    "            \"name\": \"build_text_analysis_result\",\n",
    "            \"description\": \"build the text analysis object\",\n",
    "            \"input_schema\": text_analysis_schema\n",
    "        }\n",
    "    ]\n",
    " \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1200,\n",
    "        temperature=0.2,\n",
    "        system=\"You are analyzing the sentiment of a text.\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{text}\"\n",
    "            }\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice={\"type\": \"tool\", \"name\": \"build_text_analysis_result\"}\n",
    "    )\n",
    " \n",
    "    function_call = message.content[0].input\n",
    "    return TextAnalysis(**function_call)\n",
    "\n",
    "# test it out!\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "sample_text = \"Claude tool use can be quite useful if used correctly.\"\n",
    " \n",
    "print(f\"Text: {sample_text}\\nAnalyzing text...\\n\")\n",
    "analysis = analyze_text_with_claude(api_key, sample_text)\n",
    "print(f\"Sentiment: {analysis.sentiment}\")\n",
    "print(f\"Main topics: {', '.join(analysis.main_topics)}\")\n",
    "print(f\"Word count: {analysis.word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic models:\n",
      "- claude-3-5-sonnet-20241022\n",
      "- claude-3-5-haiku-20241022\n",
      "- claude-3-5-sonnet-20240620\n",
      "- claude-3-haiku-20240307\n",
      "- claude-3-opus-20240229\n",
      "- claude-3-sonnet-20240229\n",
      "- claude-2.1\n",
      "- claude-2.0\n"
     ]
    }
   ],
   "source": [
    "def list_anthropic_models(api_key: str) -> List[str]:\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    models = client.models.list()\n",
    "    return [model.id for model in models]  # Use id instead of name\n",
    "\n",
    "# Test it out\n",
    "models = list_anthropic_models(os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "print(\"Available Anthropic models:\")\n",
    "for model in models:\n",
    "    print(f\"- {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's add our Pydantic models\n",
    "class Step(BaseModel):\n",
    "    action: str\n",
    "    inputs: List[str]\n",
    "    outputs: List[str]\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "\n",
    "class ActionPattern(BaseModel):\n",
    "    type: str\n",
    "    frequency: str\n",
    "    steps: List[Step]\n",
    "\n",
    "class CurrentState(BaseModel):\n",
    "    resources: dict = Field(default_factory=dict)\n",
    "    constraints: dict = Field(default_factory=dict)\n",
    "    progress_metrics: dict = Field(default_factory=dict)\n",
    "\n",
    "class ExecutionStrategy(BaseModel):\n",
    "    cycle: str\n",
    "    checkpoints: List[str]\n",
    "    memory_requirements: List[str]\n",
    "\n",
    "class TaskPlan(BaseModel):\n",
    "    goal: str\n",
    "    success_metric: str\n",
    "    current_state: CurrentState\n",
    "    execution_strategy: ExecutionStrategy\n",
    "    action_patterns: List[ActionPattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fix the generate_task_plan function\n",
    "def generate_task_plan(api_key: str, task_description: str) -> TaskPlan:\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    task_plan_schema = TaskPlan.model_json_schema()\n",
    "    \n",
    "    tools = [\n",
    "        {\n",
    "            \"name\": \"build_task_plan\",\n",
    "            \"description\": \"Build a structured execution plan for any task with clear steps and requirements\",\n",
    "            \"input_schema\": task_plan_schema\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1500,\n",
    "        temperature=0.2,\n",
    "        system=\"You are a task planning expert who breaks down complex tasks into detailed, actionable steps.\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Create a detailed execution plan for the following task: {task_description}\"\n",
    "            }\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice={\"type\": \"tool\", \"name\": \"build_task_plan\"}\n",
    "    )\n",
    "\n",
    "    return TaskPlan(**message.content[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Research and compile healthy vegetarian dinner recipes for a week\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"goal\": \"Research and compile healthy vegetarian dinner recipes for a week\",\n",
      "  \"success_metric\": \"Have a set of 7 healthy vegetarian dinner recipes to cook for the week\",\n",
      "  \"current_state\": {\n",
      "    \"resources\": {\n",
      "      \"time_available\": \"1 week\",\n",
      "      \"cooking_experience\": \"intermediate\"\n",
      "    },\n",
      "    \"constraints\": {\n",
      "      \"dietary_restrictions\": \"vegetarian\"\n",
      "    },\n",
      "    \"progress_metrics\": {\n",
      "      \"number_of_recipes_found\": 0\n",
      "    }\n",
      "  },\n",
      "  \"execution_strategy\": {\n",
      "    \"cycle\": \"daily\",\n",
      "    \"checkpoints\": [\n",
      "      \"Identify recipe sources\",\n",
      "      \"Compile list of potential recipes\",\n",
      "      \"Evaluate recipes for health and feasibility\",\n",
      "      \"Select 7 recipes\"\n",
      "    ],\n",
      "    \"memory_requirements\": [\n",
      "      \"list of recipe sources\",\n",
      "      \"list of potential recipes\",\n",
      "      \"evaluation criteria for recipes\"\n",
      "    ]\n",
      "  },\n",
      "  \"action_patterns\": [\n",
      "    {\n",
      "      \"type\": \"Research\",\n",
      "      \"frequency\": \"daily\",\n",
      "      \"steps\": [\n",
      "        {\n",
      "          \"action\": \"Identify online recipe sources\",\n",
      "          \"inputs\": [],\n",
      "          \"outputs\": [\n",
      "            \"list of recipe websites\"\n",
      "          ],\n",
      "          \"constraints\": [\n",
      "            \"vegetarian\",\n",
      "            \"healthy\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"action\": \"Search for potential recipes\",\n",
      "          \"inputs\": [\n",
      "            \"list of recipe websites\"\n",
      "          ],\n",
      "          \"outputs\": [\n",
      "            \"list of potential recipes\"\n",
      "          ],\n",
      "          \"constraints\": [\n",
      "            \"vegetarian\",\n",
      "            \"healthy\",\n",
      "            \"dinner\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"action\": \"Evaluate recipes for health and feasibility\",\n",
      "          \"inputs\": [\n",
      "            \"list of potential recipes\"\n",
      "          ],\n",
      "          \"outputs\": [\n",
      "            \"list of selected recipes\"\n",
      "          ],\n",
      "          \"constraints\": [\n",
      "            \"vegetarian\",\n",
      "            \"healthy\",\n",
      "            \"dinner\",\n",
      "            \"ingredients available\",\n",
      "            \"preparation time\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Compilation\",\n",
      "      \"frequency\": \"end of week\",\n",
      "      \"steps\": [\n",
      "        {\n",
      "          \"action\": \"Select 7 recipes from the list\",\n",
      "          \"inputs\": [\n",
      "            \"list of selected recipes\"\n",
      "          ],\n",
      "          \"outputs\": [\n",
      "            \"set of 7 recipes\"\n",
      "          ],\n",
      "          \"constraints\": [\n",
      "            \"vegetarian\",\n",
      "            \"healthy\",\n",
      "            \"dinner\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"Research and compile healthy vegetarian dinner recipes for a week\"\n",
    "print(f\"\\nTask: {task}\")\n",
    "print(\"-\" * 50)\n",
    "plan = generate_task_plan(os.getenv(\"ANTHROPIC_API_KEY\"), task)\n",
    "print(json.dumps(plan.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build an agentic agent that can do tasks by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing basic printing...\n",
      "Hello world!\n",
      "Numbers: 2\n",
      "Multiple\n",
      "lines\n",
      "test\n",
      "\n",
      "\n",
      "Testing directory listing...\n",
      "Current directory contents:\n",
      "hello.ipynb\n",
      "\n",
      "\n",
      "Testing system memory info...\n",
      "Total RAM: 16.0 GB\n",
      "Available RAM: 3.0 GB\n",
      "RAM Usage: 81.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from standardbackend.tools.python_code_runner import execute_python_code, EvalInput\n",
    "\n",
    "# Test printing\n",
    "print(\"Testing basic printing...\")\n",
    "r = execute_python_code(EvalInput(code=\"print('Hello world!')\\nprint(f'Numbers: {1+1}')\\nprint('Multiple\\\\nlines\\\\ntest')\"))\n",
    "print(r)\n",
    "\n",
    "# Test directory listing\n",
    "print(\"\\nTesting directory listing...\")\n",
    "r = execute_python_code(EvalInput(code=\"\"\"\n",
    "import os\n",
    "print('Current directory contents:')\n",
    "print('\\\\n'.join(os.listdir('.')))\n",
    "\"\"\"))\n",
    "print(r)\n",
    "\n",
    "# Test system RAM info\n",
    "print(\"\\nTesting system memory info...\")\n",
    "r = execute_python_code(EvalInput(code=\"\"\"\n",
    "import psutil\n",
    "mem = psutil.virtual_memory()\n",
    "print(f'Total RAM: {mem.total / (1024**3):.1f} GB')\n",
    "print(f'Available RAM: {mem.available / (1024**3):.1f} GB') \n",
    "print(f'RAM Usage: {mem.percent}%')\n",
    "\"\"\"))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use tools?\n",
    "\n",
    "Extract tool input(s), run code, and return results: (API request)\n",
    "\n",
    "On the client side, you should extract the tool name and input(s) from Claude's tool use request.\n",
    "Run the actual tool code on the client side.\n",
    "Return the results to Claude by continuing the conversation with a new user message containing a tool_result content block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm.. do we want to have threads be a composite of Message and json types?\n",
    "\n",
    "```\n",
    "Message(id='msg_018XHYaQRDBURUvsCTY5wyRd', content=[TextBlock(text='Here is how we can check the amount of memory available in the current environment:', type='text'), ToolUseBlock(id='toolu_01Q8sFHK3EnUoohV1vfzHNf6', input={'code': 'import psutil\\n\\ntotal_memory = psutil.virtual_memory().total\\nprint(f\"Total memory: {total_memory / (1024 ** 2):.2f} MB\")'}, name='run_python_code', type='tool_use')], model='claude-3-haiku-20240307', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=439, output_tokens=116))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tool_use] run_python_code toolu_015aAs4Ly2Xj1KXGqJJXQ2vh\n",
      "=>  {'code': 'import psutil\\n\\nprint(f\"Total memory: {psutil.virtual_memory().total / (1024.0 ** 2):.2f} MB\")\\nprint(f\"Available memory: {psutil.virtual_memory().available / (1024.0 ** 2):.2f} MB\")', 'max_output_length': 100, 'timeout': 10}\n",
      "[tool_answer] Total memory: 16384.00 MB\n",
      "Available memory: 3481.84 MB\n",
      "\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "How much memory do I have right now? Be sure to use the tool!\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "Okay, let's use the Python code tool to check how much memory you have available right now:\n",
      "\u001b[1m\u001b[35m\n",
      "[Tool Use]\u001b[0m\n",
      "Tool: run_python_code\n",
      "Input: {'code': 'import psutil\\n\\nprint(f\"Total memory: {psutil.virtual_memory().total / (1024.0 ** 2):.2f} MB\")\\nprint(f\"Available memory: {psutil.virtual_memory().available / (1024.0 ** 2):.2f} MB\")', 'max_output_length': 100, 'timeout': 10}\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "\u001b[1m\u001b[33m\n",
      "[Tool Result]\u001b[0m\n",
      "Total memory: 16384.00 MB\n",
      "Available memory: 3481.84 MB\n",
      "\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "The output shows that your total system memory is 16384 MB (16 GB), and the currently available memory is 3481.84 MB. This can vary depending on what other applications and processes are running on your system at the moment.\n"
     ]
    }
   ],
   "source": [
    "from standardbackend.helpers.thread import Thread\n",
    "from standardbackend.utils import pretty_print_messages\n",
    "\n",
    "# Basic usage - checking system memory\n",
    "thread = Thread(on_tool_use_callback=\"default\")\n",
    "messages = thread.send_message(\"How much memory do I have right now? Be sure to use the tool!\")\n",
    "pretty_print_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tool_answer] Current CPU usage: 27.5%\n",
      "\n",
      "[tool_answer] Yes, it's a good time to run a heavy computation task.\n",
      "\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "What's my current CPU usage? Use the tool!\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "Here is how we can check the current CPU usage in Python:\n",
      "\u001b[1m\u001b[35m\n",
      "[Tool Use]\u001b[0m\n",
      "Tool: run_python_code\n",
      "Input: {'code': 'import psutil\\n\\ncpu_percent = psutil.cpu_percent(interval=1)\\nprint(f\"Current CPU usage: {cpu_percent}%\")', 'max_output_length': 100, 'timeout': 10}\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "\u001b[1m\u001b[33m\n",
      "[Tool Result]\u001b[0m\n",
      "Current CPU usage: 27.5%\n",
      "\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "The key steps are:\n",
      "\n",
      "1. Import the `psutil` module, which provides cross-platform APIs for retrieving information about running processes and system utilization.\n",
      "2. Use the `psutil.cpu_percent()` function to get the current CPU usage percentage. We pass `interval=1` to get the usage over the last 1 second.\n",
      "3. Print the CPU usage percentage.\n",
      "\n",
      "This gives us the current CPU utilization on the system. Let me know if you need any clarification or have additional questions!\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "Based on the CPU usage you just checked, would you recommend running a heavy computation task right now? Why? Use the tool!\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "To determine if it's a good time to run a heavy computation task based on the current CPU usage, we can analyze the CPU usage percentage:\n",
      "\u001b[1m\u001b[35m\n",
      "[Tool Use]\u001b[0m\n",
      "Tool: run_python_code\n",
      "Input: {'code': 'import psutil\\n\\ncpu_percent = psutil.cpu_percent(interval=1)\\n\\nif cpu_percent < 50:\\n    print(\"Yes, it\\'s a good time to run a heavy computation task.\")\\nelse:\\n    print(\"No, the CPU is already heavily utilized. Running a heavy task now may cause performance issues.\")', 'max_output_length': 200, 'timeout': 10}\n",
      "\u001b[1m\u001b[32m\n",
      "[User]\u001b[0m\n",
      "\u001b[1m\u001b[33m\n",
      "[Tool Result]\u001b[0m\n",
      "Yes, it's a good time to run a heavy computation task.\n",
      "\n",
      "\u001b[1m\u001b[34m\n",
      "[Assistant]\u001b[0m\n",
      "The key points are:\n",
      "\n",
      "1. We measured the current CPU usage and stored it in the `cpu_percent` variable.\n",
      "2. We then checked if the CPU usage is less than 50%. This is a reasonable threshold to indicate that the system has enough available CPU capacity to handle an additional heavy task.\n",
      "3. Since the current CPU usage is 27.5%, which is less than 50%, we can conclude that it's a good time to run a heavy computation task without causing performance issues.\n",
      "\n",
      "The reasoning is that if the CPU is already heavily utilized (e.g., over 50%), adding a new heavy task may cause the system to become overloaded, leading to slower performance, potential crashes, or other issues. However, with the current 27.5% CPU usage, there is enough headroom to handle the additional computational load.\n",
      "\n",
      "So based on the information provided, I would recommend running the heavy computation task now, as the system has sufficient CPU resources available.\n"
     ]
    }
   ],
   "source": [
    "# Reload the thread module to get latest changes\n",
    "from importlib import reload\n",
    "from standardbackend.helpers import thread\n",
    "reload(thread)\n",
    "from standardbackend.helpers.thread import Thread\n",
    "\n",
    "# Chain of tool interactions\n",
    "analysis_thread = Thread()\n",
    "messages = analysis_thread.send_message(\"What's my current CPU usage? Use the tool!\")\n",
    "# Then ask for analysis of that data\n",
    "messages = analysis_thread.send_message(\"Based on the CPU usage you just checked, would you recommend running a heavy computation task right now? Why? Use the tool!\")\n",
    "pretty_print_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tool toolu_01HuUk9eEgP9zKGnwYmJjsx2 failed with error: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool toolu_01HuUk9eEgP9zKGnwYmJjsx2 failed with error: None\n",
      "[tool_answer] {'id': '1213210', 'name': 'John Doe', 'email': 'john@gmail.com', 'phone': '123-456-7890', 'username': 'johndoe'}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your API request included an `assistant` message in the final position, which would pre-fill the `assistant` response. When using tools, pre-filling the `assistant` response is not supported.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 278\u001b[0m\n\u001b[1;32m    251\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    252\u001b[0m     Tool(\n\u001b[1;32m    253\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_user\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     ),\n\u001b[1;32m    276\u001b[0m ]\n\u001b[1;32m    277\u001b[0m t \u001b[38;5;241m=\u001b[39m Thread(tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[0;32m--> 278\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you look up my orders? My email is john@gmail.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m pretty_print_messages(messages)\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/src/standardbackend/helpers/thread.py:133\u001b[0m, in \u001b[0;36mThread.send_message\u001b[0;34m(self, message, tool_mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mextend(tool_responses)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m claude_message\u001b[38;5;241m.\u001b[39mstop_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_use\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m     claude_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     metadata, tool_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_message(claude_message)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    143\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: claude_message\u001b[38;5;241m.\u001b[39mcontent}\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/.venv/lib/python3.12/site-packages/anthropic/resources/messages/messages.py:901\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    895\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    898\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    899\u001b[0m     )\n\u001b[0;32m--> 901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1279\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1267\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1276\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1277\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1278\u001b[0m     )\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CascadeProjects/infinityagents/standardbackend/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1060\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1059\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1063\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1069\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your API request included an `assistant` message in the final position, which would pre-fill the `assistant` response. When using tools, pre-filling the `assistant` response is not supported.'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "from standardbackend import Thread, Agent\n",
    "\n",
    "a = Agent(name=\"Sam\", prompt=sam_prompt)\n",
    "\n",
    "t = Thread(agent=a)\n",
    "\n",
    "t.send_message(\"hey, what's the weather in LA?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Can you look up my orders? My email is john@gmail.com'},\n",
       " {'role': 'assistant',\n",
       "  'content': [TextBlock(text='Okay, let me look up your orders using your email address.', type='text'),\n",
       "   ToolUseBlock(id='toolu_01HuUk9eEgP9zKGnwYmJjsx2', input={'key': 'email', 'value': 'john@gmail.com'}, name='get_user', type='tool_use')]}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-haiku-20241022\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
